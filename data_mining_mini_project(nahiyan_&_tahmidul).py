# -*- coding: utf-8 -*-
"""Data Mining Mini Project(Nahiyan & Tahmidul).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Siy4Emsz6PgLXVQ8eS1yJk5WrQy4pOD
"""

!pip install -q kaggle

from google.colab import files

# Upload your Kaggle API key (kaggle.json) file
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list

!kaggle datasets download -d omermetinn/tweets-about-the-top-companies-from-2015-to-2020
!kaggle datasets download -d suyashlakhani/apple-stock-prices-20152020

!mkdir tweets

!unzip tweets-about-the-top-companies-from-2015-to-2020.zip -d tweets
!unzip apple-stock-prices-20152020.zip -d tweets

cd/content/tweets/

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.preprocessing import  LabelEncoder, MinMaxScaler, StandardScaler
import datetime as dt

from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
import lightgbm as lgb

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Uyarıları görmezden gelme!
import warnings
warnings.filterwarnings("ignore")

# Veri setinin python ile okunması!
df = pd.read_csv("AAPL.csv")
df.head()

# İşimize yaramayacak kolonları çıkarıyoruz!
df = df.drop(["Unnamed: 0", "symbol", "divCash", "splitFactor"], axis = 1)
df.head()

# date değeri olmadan ayrı bir veri seti oluyoruz!
df_not_date = df.drop(["date"], axis=1)
df_not_date.head()

# İşlev, veri kümesini analiz etmek içindir.
def InfofData(dataframe):
    print("############## Head ##############")
    print(dataframe.head())
    print("\n")
    print("############## Tail ##############")
    print(dataframe.head())
    print("\n")
    print("############## Info ##############")
    print(dataframe.info())
    print("\n")
    print("############## Shape ##############")
    print(dataframe.shape)
    print("\n")
    print("############## Columns Name ##############")
    print(dataframe.columns)
    print("\n")
    print("############## Describe ##############")
    print(dataframe.describe().T)
    print("\n")
    print("############## Null ##############")
    print(dataframe.isnull().sum())

InfofData(df_not_date)

f,ax=plt.subplots(figsize=(13,10))
sns.heatmap(df_not_date.corr(),annot=True,linewidths=0.5,linecolor="red",fmt=".4f",ax=ax)
plt.show()

sns.pairplot(df_not_date)

# Aykırı değerlerin grafik olarak görselleştirilmesi
for i in df_not_date.columns:
    plt.figure(figsize = (10,2))
    sns.boxplot(x = df[i])
    plt.show()

# Aykırı değer fonkisyonlarım
def outlier_thresholds(dataframe, col_name, q1 = 0.05, q3 = 0.95):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def check_outlier(dataframe, col_name):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis = None):
        return True
    else:
        return False

# Kolonlar arasındaki aykırı değerlere bakıyorum.
for i in df_not_date.columns:
    check_value = check_outlier(df_not_date, i)
    print(f"{i} kolonun aykırı değer durumu: {check_value}")

# Aykırı değerlleri değiştireceğim fonksiyonum
def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit

# Aykırı değer aralıklarını değiştirdiğim kısım!
for i in df_not_date.columns:
    check_value = check_outlier(df_not_date, i)
    if check_value:
        replace_with_thresholds(df_not_date, i)

# Kolonlar arasındaki aykırı değerlere bakıyorum.
for i in df_not_date.columns:
    check_value = check_outlier(df_not_date, i)
    print(f"{i} kolonun aykırı değer durumu: {check_value}")

mms = MinMaxScaler()
mms_df = mms.fit_transform(df_not_date)

df_mms = pd.DataFrame(data = mms_df, columns = df_not_date.columns)
df_mms.head()

X = df_mms.drop(["close"], axis = 1)
y = df_mms["close"]

X_train = X[:943]
X_test = X[943:]
Y_train = y[:943]
Y_test = y[943:]

X_train.head()

Y_train.head()

print("X_train shape: ", X_train.shape)
print("X_test shape: ", X_test.shape)
print("Y_train shape: ", Y_train.shape)
print("Y_test shape: ", Y_test.shape)

# Bağımlı değişkenimizin tüm hali
plt.plot(df_mms["close"])
plt.show()

# Bağımlı değişkenimizin Y_train ve Y_test olarak ayrılmı hali
plt.plot(df_mms["close"][:943])
plt.plot(df_mms["close"][943:], color = "green")
plt.show()

RanForReg_model = RandomForestRegressor()

"""
# Bu parametreler ile Skorumuz:  0.8651725640352754
parameters_rfr = {
    'n_estimators': [100, 200, 300, 500],  # Ağaç sayısı
    'max_depth': [None, 5, 10, 20],  # Ağaçların maksimum derinliği
    'min_samples_split': [2, 5, 10, 15],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['auto', 'sqrt', 'log2']  # Özelliklerin seçiminde kullanılacak strateji
}
"""

# Bu parametreler ile skorumuz: 0.8660908923478665
parameters_rfr = {
    'n_estimators': [100, 200, 300, 400],  # Ağaç sayısı
    'max_depth': [None, 10, 15, 20],  # Ağaçların maksimum derinliği
    'min_samples_split': [2, 5, 10],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['auto', 'sqrt']  # Özelliklerin seçiminde kullanılacak strateji
}


gsc = GridSearchCV(estimator = RanForReg_model, param_grid = parameters_rfr, scoring='r2', cv=5)
gsc_search = gsc.fit(X_train, Y_train)
y_pred_gsc = gsc.predict(X_test)

best_parm_gsc_rfr = gsc_search.best_params_
best_score_gsc_rfr = gsc_search.best_score_

print("GridSearch ile Random Forest Regression modelinin en iyi parametırları: ", best_parm_gsc_rfr)
print("GridSearch ile Random Forest Regression modelinin en iyi skoru: ",best_score_gsc_rfr)

best_model_rfr = RandomForestRegressor(**best_parm_gsc_rfr)
best_model_rfr.fit(X_train, Y_train)

predictions_rfr = best_model_rfr.predict(X_test)

plt.plot(df_mms["close"][:943])
plt.plot(df_mms["close"][943:], color = "green")
plt.plot(range(943,len(X)), predictions_rfr, color = "red")
plt.show()

dtr_model = DecisionTreeRegressor()

parameters_dtr = {
    'max_depth': [10, 15, 20],  # Ağacın maksimum derinliği
    'min_samples_split': [2, 5, 10],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['log2', 'sqrt', None],  # Özelliklerin seçiminde kullanılacak strateji
    'criterion': ['friedman_mse', 'squared_error', 'absolute_error', 'poisson']  # Bölme kriteri
}

gsc_dtr = GridSearchCV(estimator = dtr_model, param_grid = parameters_dtr, scoring='r2', cv=5)
gsc_search_dtr = gsc_dtr.fit(X_train, Y_train)
y_pred_gsc_dtr = gsc_dtr.predict(X_test)

best_parm_gsc_dtr = gsc_search_dtr.best_params_
best_score_gsc_dtr = gsc_search_dtr.best_score_

print("GridSearch ile Decision Tree Regression modelinin en iyi parametırları: ", best_parm_gsc_dtr)
print("GridSearch ile Decision Tree Regression modelinin en iyi skoru: ",best_score_gsc_dtr)

# En iyi parametrelerle modeli yeniden eğitme
best_model_dtr = DecisionTreeRegressor(**best_parm_gsc_dtr)
best_model_dtr.fit(X_train, Y_train)

# Tahmin yapma
predictions_dtr = best_model_dtr.predict(X_test)

plt.plot(df_mms["close"][:943])
plt.plot(df_mms["close"][943:], color = "green")
plt.plot(range(943,len(X)), predictions_dtr, color = "red")
plt.show()

xgb_model = xgb.XGBRegressor()


"""
# Bu parametreler ile sonucum: 0.8675297699269745
parameters_xgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [7, 9, 11],  # Ağacın maksimum derinliği
    'n_estimators': [100, 200, 300],  # Ağaç sayısı
    'subsample': [0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.8, 1.0]  # Özellik örnekleme oranı
}
"""
# Bu parametreler ile sonucum: 0.8675297699269745
parameters_xgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [7, 9, 11],  # Ağacın maksimum derinliği
    'n_estimators': [100, 200, 300],  # Ağaç sayısı
    'subsample': [0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.8, 1.0],  # Özellik örnekleme oranı
    'gamma': [0, 0.1, 0.3]  # Kesme parametresi
}

gsc_xgb = GridSearchCV(estimator = xgb_model, param_grid = parameters_xgb, scoring='r2', cv=5)
gsc_search_xgb = gsc_xgb.fit(X_train, Y_train)
y_pred_gsc_xgb = gsc_xgb.predict(X_test)

best_parm_gsc_xgb = gsc_search_xgb.best_params_
best_score_gsc_xgb = gsc_search_xgb.best_score_

print("GridSearch ile XGBoost modelinin en iyi parametırları: ", best_parm_gsc_xgb)
print("GridSearch ile XGBoost modelinin en iyi skoru: ",best_score_gsc_xgb)

# En iyi parametrelerle modeli yeniden eğitme
best_model_xgb = xgb.XGBRegressor(**best_parm_gsc_xgb)
best_model_xgb.fit(X_train, Y_train)

# Tahmin yapma
predictions_xgb = best_model_xgb.predict(X_test)

plt.plot(df_mms["close"][:943])
plt.plot(df_mms["close"][943:], color = "green")
plt.plot(range(943,len(X)), predictions_xgb, color = "red")
plt.show()

lgb_model = lgb.LGBMRegressor()

"""
# Bu parametreler ile skorum: 0.8265849399340167
parameters_lgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [3, 5, 7],  # Ağacın maksimum derinliği
    'num_leaves': [10, 20, 30],  # Yaprak sayısı
    'subsample': [0.6, 0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.6, 0.8, 1.0],  # Özellik örnekleme oranı
    'reg_alpha': [0, 0.1, 0.5],  # L1 düzenlileştirme parametresi
    'reg_lambda': [0, 0.1, 0.5]  # L2 düzenlileştirme parametresi
}
"""
# Bu parametreler ile skorum: 0.8265849399340167
parameters_lgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [3, 5, 7],  # Ağacın maksimum derinliği
    'num_leaves': [10, 20, 30],  # Yaprak sayısı
    'subsample': [0.6, 0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.6, 0.8, 1.0],  # Özellik örnekleme oranı
    'reg_alpha': [0, 0.1, 0.5],  # L1 düzenlileştirme parametresi
    'reg_lambda': [0, 0.1, 0.5],  # L2 düzenlileştirme parametresi
    'min_child_samples': [20, 50, 100]  # Minimum çocuk örnek sayısı
}

gsc_lgb = GridSearchCV(estimator = lgb_model, param_grid = parameters_lgb, scoring = "r2", cv = 5)
gsc_search_lgb = gsc_lgb.fit(X_train, Y_train)
y_pred_gsc_lgb = gsc_lgb.predict(X_test)

best_parm_gsc_lgb = gsc_search_lgb.best_params_
best_score_gsc_lgb = gsc_search_lgb.best_score_

print("GridSearch ile LightGBM modelinin en iyi parametırları: ", best_parm_gsc_lgb)
print("GridSearch ile LightGBM modelinin en iyi skoru: ",best_score_gsc_lgb)

# En iyi parametrelerle modeli yeniden eğitme
best_model_lgb = lgb.LGBMRegressor(**best_parm_gsc_lgb)
best_model_lgb.fit(X_train, Y_train)

# Tahmin yapma
predictions_lgb = best_model_lgb.predict(X_test)

plt.plot(df_mms["close"][:943])
plt.plot(df_mms["close"][943:], color = "green")
plt.plot(range(943,len(X)), predictions_lgb, color = "red")
plt.show()

# date değeri olmadan ayrı bir veri seti oluyoruz!
# İkinci aşamada kullancağım veri setinin ilk hali!
df_not_date = df.drop(["date"], axis=1)
df_not_date.head()

X = df_not_date.drop(["close"], axis = 1)
y = df_not_date["close"]

# Bu aşamada da veri setinin %75 Train %25 Test olarak alacağım
X_train = X[:943]
X_test = X[943:]
Y_train = y[:943]
Y_test = y[943:]

X_train.head()

Y_train.head()

print("X_train shape: ", X_train.shape)
print("X_test shape: ", X_test.shape)
print("Y_train shape: ", Y_train.shape)
print("Y_test shape: ", Y_test.shape)

# Bağımlı değişkenimizin tüm hali
plt.plot(df_not_date["close"])
plt.show()

# Bağımlı değişkenimizin Y_train ve Y_test olarak ayrılmı hali
plt.plot(df_not_date["close"][:943])
plt.plot(df_not_date["close"][943:], color = "green")
plt.show()

RanForReg_model = RandomForestRegressor()

parameters_rfr = {
    'n_estimators': [100, 200, 300, 400],  # Ağaç sayısı
    'max_depth': [None, 10, 15, 20],  # Ağaçların maksimum derinliği
    'min_samples_split': [2, 5, 10],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['auto', 'sqrt']  # Özelliklerin seçiminde kullanılacak strateji
}


gsc = GridSearchCV(estimator = RanForReg_model, param_grid = parameters_rfr, scoring='r2', cv=5)
gsc_search = gsc.fit(X_train, Y_train)
y_pred_gsc = gsc.predict(X_test)

best_parm_gsc_rfr = gsc_search.best_params_
best_score_gsc_rfr = gsc_search.best_score_

print("GridSearch ile Random Forest Regression modelinin en iyi parametırları: ", best_parm_gsc_rfr)
print("GridSearch ile Random Forest Regression modelinin en iyi skoru: ",best_score_gsc_rfr)

best_model_rfr = RandomForestRegressor(**best_parm_gsc_rfr)
best_model_rfr.fit(X_train, Y_train)

predictions_rfr = best_model_rfr.predict(X_test)

plt.plot(df_not_date["close"][:943])
plt.plot(df_not_date["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_rfr, color = "red")
plt.show()

dtr_model = DecisionTreeRegressor()

parameters_dtr = {
    'max_depth': [10, 15, 20],  # Ağacın maksimum derinliği
    'min_samples_split': [2, 5, 10],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['log2', 'sqrt', None],  # Özelliklerin seçiminde kullanılacak strateji
    'criterion': ['friedman_mse', 'squared_error', 'absolute_error', 'poisson']  # Bölme kriteri
}

gsc_dtr = GridSearchCV(estimator = dtr_model, param_grid = parameters_dtr, scoring='r2', cv=5)
gsc_search_dtr = gsc_dtr.fit(X_train, Y_train)
y_pred_gsc_dtr = gsc_dtr.predict(X_test)

best_parm_gsc_dtr = gsc_search_dtr.best_params_
best_score_gsc_dtr = gsc_search_dtr.best_score_

print("GridSearch ile Decision Tree Regression modelinin en iyi parametırları: ", best_parm_gsc_dtr)
print("GridSearch ile Decision Tree Regression modelinin en iyi skoru: ",best_score_gsc_dtr)

# En iyi parametrelerle modeli yeniden eğitme
best_model_dtr = DecisionTreeRegressor(**best_parm_gsc_dtr)
best_model_dtr.fit(X_train, Y_train)

# Tahmin yapma
predictions_dtr = best_model_dtr.predict(X_test)

plt.plot(df_not_date["close"][:943])
plt.plot(df_not_date["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_dtr, color = "red")
plt.show()

xgb_model = xgb.XGBRegressor()

parameters_xgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [7, 9, 11],  # Ağacın maksimum derinliği
    'n_estimators': [100, 200, 300],  # Ağaç sayısı
    'subsample': [0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.8, 1.0],  # Özellik örnekleme oranı
    'gamma': [0, 0.1, 0.3]  # Kesme parametresi
}

gsc_xgb = GridSearchCV(estimator = xgb_model, param_grid = parameters_xgb, scoring='r2', cv=5)
gsc_search_xgb = gsc_xgb.fit(X_train, Y_train)
y_pred_gsc_xgb = gsc_xgb.predict(X_test)

best_parm_gsc_xgb = gsc_search_xgb.best_params_
best_score_gsc_xgb = gsc_search_xgb.best_score_

print("GridSearch ile XGBoost modelinin en iyi parametırları: ", best_parm_gsc_xgb)
print("GridSearch ile XGBoost modelinin en iyi skoru: ",best_score_gsc_xgb)

# En iyi parametrelerle modeli yeniden eğitme
best_model_xgb = xgb.XGBRegressor(**best_parm_gsc_xgb)
best_model_xgb.fit(X_train, Y_train)

# Tahmin yapma
predictions_xgb = best_model_xgb.predict(X_test)

plt.plot(df_not_date["close"][:943])
plt.plot(df_not_date["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_xgb, color = "red")
plt.show()

lgb_model = lgb.LGBMRegressor()

parameters_lgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [3, 5, 7],  # Ağacın maksimum derinliği
    'num_leaves': [10, 20, 30],  # Yaprak sayısı
    'subsample': [0.6, 0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.6, 0.8, 1.0],  # Özellik örnekleme oranı
    'reg_alpha': [0, 0.1, 0.5],  # L1 düzenlileştirme parametresi
    'reg_lambda': [0, 0.1, 0.5],  # L2 düzenlileştirme parametresi
    'min_child_samples': [20, 50, 100]  # Minimum çocuk örnek sayısı
}

gsc_lgb = GridSearchCV(estimator = lgb_model, param_grid = parameters_lgb, scoring = "r2", cv = 5)
gsc_search_lgb = gsc_lgb.fit(X_train, Y_train)
y_pred_gsc_lgb = gsc_lgb.predict(X_test)

best_parm_gsc_lgb = gsc_search_lgb.best_params_
best_score_gsc_lgb = gsc_search_lgb.best_score_

print("GridSearch ile LightGBM modelinin en iyi parametırları: ", best_parm_gsc_lgb)
print("GridSearch ile LightGBM modelinin en iyi skoru: ",best_score_gsc_lgb)

# En iyi parametrelerle modeli yeniden eğitme
best_model_lgb = lgb.LGBMRegressor(**best_parm_gsc_lgb)
best_model_lgb.fit(X_train, Y_train)

# Tahmin yapma
predictions_lgb = best_model_lgb.predict(X_test)

plt.plot(df_not_date["close"][:943])
plt.plot(df_not_date["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_lgb, color = "red")
plt.show()

# date değeri olmadan ayrı bir veri seti oluyoruz!
# İkinci aşamada kullancağım veri setinin ilk hali!
df_not_date = df.drop(["date"], axis=1)
df_not_date.head()

ss = StandardScaler()
df_ss = ss.fit_transform(df_not_date)

df_ss = pd.DataFrame(data = df_ss, columns = df_not_date.columns)

df_ss.head()

X = df_ss.drop(["close"], axis = 1)
y = df_ss["close"]

# Bu aşamada da veri setinin %75 Train %25 Test olarak alacağım
X_train = X[:943]
X_test = X[943:]
Y_train = y[:943]
Y_test = y[943:]

print("X_train shape: ", X_train.shape)
print("X_test shape: ", X_test.shape)
print("Y_train shape: ", Y_train.shape)
print("Y_test shape: ", Y_test.shape)

# Bağımlı değişkenimizin Y_train ve Y_test olarak ayrılmı hali
plt.plot(df_ss["close"][:943])
plt.plot(df_ss["close"][943:], color = "green")
plt.show()

RanForReg_model = RandomForestRegressor()

parameters_rfr = {
    'n_estimators': [100, 200, 300, 400],  # Ağaç sayısı
    'max_depth': [None, 10, 15, 20],  # Ağaçların maksimum derinliği
    'min_samples_split': [2, 5, 10],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['auto', 'sqrt']  # Özelliklerin seçiminde kullanılacak strateji
}


gsc = GridSearchCV(estimator = RanForReg_model, param_grid = parameters_rfr, scoring='r2', cv=5)
gsc_search = gsc.fit(X_train, Y_train)
y_pred_gsc = gsc.predict(X_test)

best_parm_gsc_rfr = gsc_search.best_params_
best_score_gsc_rfr = gsc_search.best_score_

print("GridSearch ile Random Forest Regression modelinin en iyi parametırları: ", best_parm_gsc_rfr)
print("GridSearch ile Random Forest Regression modelinin en iyi skoru: ",best_score_gsc_rfr)

best_model_rfr = RandomForestRegressor(**best_parm_gsc_rfr)
best_model_rfr.fit(X_train, Y_train)

predictions_rfr = best_model_rfr.predict(X_test)

plt.plot(df_ss["close"][:943])
plt.plot(df_ss["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_rfr, color = "red")
plt.show()

dtr_model = DecisionTreeRegressor()

parameters_dtr = {
    'max_depth': [10, 15, 20],  # Ağacın maksimum derinliği
    'min_samples_split': [2, 5, 10],  # Dallanma için minimum örnek sayısı
    'min_samples_leaf': [1, 2, 4],  # Yaprak düğümleri için minimum örnek sayısı
    'max_features': ['log2', 'sqrt', None],  # Özelliklerin seçiminde kullanılacak strateji
    'criterion': ['friedman_mse', 'squared_error', 'absolute_error', 'poisson']  # Bölme kriteri
}

gsc_dtr = GridSearchCV(estimator = dtr_model, param_grid = parameters_dtr, scoring='r2', cv=5)
gsc_search_dtr = gsc_dtr.fit(X_train, Y_train)
y_pred_gsc_dtr = gsc_dtr.predict(X_test)

best_parm_gsc_dtr = gsc_search_dtr.best_params_
best_score_gsc_dtr = gsc_search_dtr.best_score_

print("GridSearch ile Decision Tree Regression modelinin en iyi parametırları: ", best_parm_gsc_dtr)
print("GridSearch ile Decision Tree Regression modelinin en iyi skoru: ",best_score_gsc_dtr)

# En iyi parametrelerle modeli yeniden eğitme
best_model_dtr = DecisionTreeRegressor(**best_parm_gsc_dtr)
best_model_dtr.fit(X_train, Y_train)

# Tahmin yapma
predictions_dtr = best_model_dtr.predict(X_test)

plt.plot(df_ss["close"][:943])
plt.plot(df_ss["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_dtr, color = "red")
plt.show()

xgb_model = xgb.XGBRegressor()

parameters_xgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [7, 9, 11],  # Ağacın maksimum derinliği
    'n_estimators': [100, 200, 300],  # Ağaç sayısı
    'subsample': [0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.8, 1.0],  # Özellik örnekleme oranı
    'gamma': [0, 0.1, 0.3]  # Kesme parametresi
}

gsc_xgb = GridSearchCV(estimator = xgb_model, param_grid = parameters_xgb, scoring='r2', cv=5)
gsc_search_xgb = gsc_xgb.fit(X_train, Y_train)
y_pred_gsc_xgb = gsc_xgb.predict(X_test)

best_parm_gsc_xgb = gsc_search_xgb.best_params_
best_score_gsc_xgb = gsc_search_xgb.best_score_

print("GridSearch ile XGBoost modelinin en iyi parametırları: ", best_parm_gsc_xgb)
print("GridSearch ile XGBoost modelinin en iyi skoru: ",best_score_gsc_xgb)

# En iyi parametrelerle modeli yeniden eğitme
best_model_xgb = xgb.XGBRegressor(**best_parm_gsc_xgb)
best_model_xgb.fit(X_train, Y_train)

# Tahmin yapma
predictions_xgb = best_model_xgb.predict(X_test)

plt.plot(df_ss["close"][:943])
plt.plot(df_ss["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_xgb, color = "red")
plt.show()

lgb_model = lgb.LGBMRegressor()

parameters_lgb = {
    'learning_rate': [0.01, 0.1, 0.3],  # Öğrenme oranı
    'max_depth': [3, 5, 7],  # Ağacın maksimum derinliği
    'num_leaves': [10, 20, 30],  # Yaprak sayısı
    'subsample': [0.6, 0.8, 1.0],  # Alt örnekleme oranı
    'colsample_bytree': [0.6, 0.8, 1.0],  # Özellik örnekleme oranı
    'reg_alpha': [0, 0.1, 0.5],  # L1 düzenlileştirme parametresi
    'reg_lambda': [0, 0.1, 0.5],  # L2 düzenlileştirme parametresi
    'min_child_samples': [20, 50, 100]  # Minimum çocuk örnek sayısı
}

gsc_lgb = GridSearchCV(estimator = lgb_model, param_grid = parameters_lgb, scoring = "r2", cv = 5)
gsc_search_lgb = gsc_lgb.fit(X_train, Y_train)
y_pred_gsc_lgb = gsc_lgb.predict(X_test)

best_parm_gsc_lgb = gsc_search_lgb.best_params_
best_score_gsc_lgb = gsc_search_lgb.best_score_

print("GridSearch ile LightGBM modelinin en iyi parametırları: ", best_parm_gsc_lgb)
print("GridSearch ile LightGBM modelinin en iyi skoru: ",best_score_gsc_lgb)

# En iyi parametrelerle modeli yeniden eğitme
best_model_lgb = lgb.LGBMRegressor(**best_parm_gsc_lgb)
best_model_lgb.fit(X_train, Y_train)

# Tahmin yapma
predictions_lgb = best_model_lgb.predict(X_test)

plt.plot(df_ss["close"][:943])
plt.plot(df_ss["close"][943:], color = "green")
plt.plot(range(944,len(X)+1), predictions_lgb, color = "red")
plt.show()

df_lstm = pd.read_csv("AAPL.csv")
df_lstm = df_lstm.drop(['symbol','Unnamed: 0','splitFactor','divCash'],axis=1)
df_lstm.head()

df_lstm.date = pd.to_datetime(df_lstm.date)
df_lstm.date = df_lstm.date.dt.date
df_lstm_date = df_lstm.date

df_lstm_train = df_lstm.loc[0:1095,['close', 'high', 'low', 'open', 'volume', 'adjClose', 'adjHigh',
       'adjLow', 'adjOpen', 'adjVolume']]
df_lstm_train['close'] = df_lstm_train['close'].astype('float64')
df_lstm_train.head()

df_lstm_test = df_lstm.loc[1095:,['close', 'high', 'low', 'open', 'volume', 'adjClose', 'adjHigh',
       'adjLow', 'adjOpen', 'adjVolume']]
df_lstm_test['close'] = df_lstm_test['close'].astype('float64')
df_lstm_test.reset_index(inplace=True)
df_lstm_test.head()

df_lstm_test = df_lstm_test.drop(['index'],axis=1)
df_lstm_test.head()

# Veri seti sıkıştırma işlemleri
scaler = StandardScaler()
df_train_scaled = scaler.fit_transform(df_lstm_train)
df_train_scaled.shape

scaler2 = StandardScaler()
df_train_scaled_target = scaler2.fit_transform(df_lstm_train[['close']])
df_train_scaled_target.shape

#Son 14 günün hisse senedi fiyatlarını kullanarak 1 gün önceden hisse senedi fiyatını tahmin etmek için veri oluşturma.
hops = 14
no_records = 1096
no_cols = 10
X_train=[]
y_train=[]
for i in range(hops,no_records):
    X_train.append(df_train_scaled[i-14:i])
    y_train.append(df_train_scaled_target[i][0])
X_train,y_train = np.array(X_train),np.array(y_train)

print(X_train.shape)
print(y_train.shape)

X_train_shape = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],X_train.shape[2]))
X_train_shape.shape

model = Sequential()
model.add(LSTM(units=50,return_sequences=True,input_shape=(hops,no_cols)))
model.add(Dropout(0.4))
model.add(LSTM(units=50))
model.add(Dropout(0.4))
model.add(Dense(1))
model.compile(optimizer='adam',loss='mean_squared_error')
history = model.fit(X_train_shape, y_train, epochs=100, batch_size=64)
# plot history
plt.plot(history.history['loss'], label='train')
#plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

df_train_last = df_lstm_train.iloc[-14:]
df_test_full = df_lstm_test
full_df = pd.concat((df_train_last,df_lstm_test),axis=0)
full_df.shape

full_df = scaler.transform(full_df)
full_df.shape

hops = 14
no_records = 177
no_cols = 10
X_train_pred=[]

for i in range(hops,no_records):
    X_train_pred.append(full_df[i-14:i])
X_train_pred = np.array(X_train_pred)
X_train_pred.shape

ytest = model.predict(X_train_pred)
yfinal_pred = scaler2.inverse_transform(ytest)

final_close_pred = pd.DataFrame(yfinal_pred)
final_close_pred.columns = ['final_close_pred']

full_final_test = pd.concat((final_close_pred,df_lstm_test),axis=1)
full_final_test = full_final_test[['open', 'final_close_pred', 'close', 'high', 'low', 'volume', 'adjClose', 'adjHigh',
       'adjLow', 'adjOpen', 'adjVolume']]
full_final_test.head()

real_price = full_final_test['close']
predicted_price = full_final_test['final_close_pred']
rmse = math.sqrt(mean_squared_error(real_price, predicted_price))
print('Test RMSE: %.3f' % rmse)

plt.figure(figsize=(15,10))
plt.plot(df_lstm["close"][:1096])
plt.plot(df_lstm["close"][1096:], color = "green")
plt.plot(range(1096,len(df_lstm)+1), full_final_test['final_close_pred'],label='forecast',color='red')
plt.title('Comparasion Between Actual and Forecasted Closing Price of Apple Stock Using LSTM Model',fontsize=20)
plt.show()

df1 = pd.read_csv("Company_Tweet.csv")
df2 = pd.read_csv("Tweet.csv")
df3 = pd.read_csv("Company.csv")

# Company_Tweet
df1.head()

# Company_Tweet Info
InfofData(df1)

# Tweet
df2.head()

# Tweet Info
InfofData(df2)

# Company
df3.head()

# Company Info
InfofData(df3)

# ilk once Apple şirketine ait olan tweetlerinin kimliklerini saptıyorum
df1_Apple = df1.loc[df1["ticker_symbol"] == "AAPL"]
print("############## Head ##############")
print(df1_Apple.head())
print("############## Value Counts ##############")
print(df1_Apple["ticker_symbol"].value_counts())

# df1 ve df2 tablolarını 'tweet_id' kolonunu kullanarak birleştiriyorum
merged_df_Apple = pd.merge(df1_Apple, df2, on='tweet_id')

# post_date saniye cinsinden kayıt aşınmış onu istediğim format olan gün ay yıl cinsine çeviriyorum
merged_df_Apple["post_date"] = pd.to_datetime(merged_df_Apple['post_date'],unit='s').dt.strftime('%Y-%m-%d')

# Son Tweet veri setinin hazırlama aşaması
data_Apple = pd.DataFrame(merged_df_Apple["post_date"].value_counts())
data_Apple = data_Apple.reset_index()
data_Apple.columns = ["date", "post_date_count"]
data_Apple.head()

InfofData(data_Apple)

df = pd.read_csv("AAPL.csv")
df = df.drop(["Unnamed: 0", "symbol", "divCash", "splitFactor"], axis = 1)
df.date = pd.to_datetime(df.date).dt.strftime('%Y-%m-%d')
#df.date = df.date.dt.date
df_date = df.date
df.head()

merge_data_tp = pd.merge(df, data_Apple, on='date', how='left')
merge_data_tp['post_date_count'].fillna(0, inplace=True)
merge_data_tp.head()

InfofData(merge_data_tp)

# Train Veri Seti
df_lstm_train2 = merge_data_tp.loc[0:1095,['close', 'high', 'low', 'open', 'volume', 'adjClose', 'adjHigh',
       'adjLow', 'adjOpen', 'adjVolume', 'post_date_count']]
df_lstm_train2['close'] = df_lstm_train2['close'].astype('float64')
df_lstm_train2.head()

# Test Veri Seti
df_lstm_test2 = merge_data_tp.loc[1095:,['close', 'high', 'low', 'open', 'volume', 'adjClose', 'adjHigh',
       'adjLow', 'adjOpen', 'adjVolume', 'post_date_count']]
df_lstm_test2['close'] = df_lstm_test2['close'].astype('float64')
df_lstm_test2.reset_index(inplace=True)
df_lstm_test2.head()

df_lstm_test2 = df_lstm_test2.drop(['index'],axis=1)
df_lstm_test2.head()

# Veri seti sıkıştırma işlemleri
scaler = StandardScaler()
df_train_scaled = scaler.fit_transform(df_lstm_train2)
df_train_scaled.shape

scaler2 = StandardScaler()
df_train_scaled_target = scaler2.fit_transform(df_lstm_train2[['close']])
df_train_scaled_target.shape

#Son 14 günün hisse senedi fiyatlarını kullanarak 1 gün önceden hisse senedi fiyatını tahmin etmek için veri oluşturma.
hops = 14
no_records = 1096
no_cols = 11
X_train2 = []
y_train2 = []
for i in range(hops,no_records):
    X_train2.append(df_train_scaled[i-14:i])
    y_train2.append(df_train_scaled_target[i][0])
X_train2,y_train2 = np.array(X_train2),np.array(y_train2)

print(X_train2.shape)
print(y_train2.shape)

X_train_shape2 = np.reshape(X_train2,(X_train2.shape[0],X_train2.shape[1],X_train2.shape[2]))
X_train_shape2.shape

model = Sequential()
model.add(LSTM(units=50,return_sequences=True,input_shape=(hops,no_cols)))
model.add(Dropout(0.4))
model.add(LSTM(units=50))
model.add(Dropout(0.4))
model.add(Dense(1))
model.compile(optimizer='adam',loss='mean_squared_error')
history = model.fit(X_train_shape2, y_train2, epochs=100, batch_size=64)
# plot history
plt.plot(history.history['loss'], label='train')
#plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

df_train_last = df_lstm_train2.iloc[-14:]
df_test_full = df_lstm_test2
full_df = pd.concat((df_train_last,df_lstm_test2),axis=0)
full_df.shape

full_df = scaler.transform(full_df)
full_df.shape

hops = 14
no_records = 177
no_cols = 11
X_train_pred2 = []

for i in range(hops,no_records):
    X_train_pred2.append(full_df[i-14:i])
X_train_pred2 = np.array(X_train_pred2)
X_train_pred2.shape

ytest = model.predict(X_train_pred2)
yfinal_pred = scaler2.inverse_transform(ytest)

final_close_pred = pd.DataFrame(yfinal_pred)
final_close_pred.columns = ['final_close_pred']

full_final_test = pd.concat((final_close_pred,df_lstm_test2),axis=1)
full_final_test = full_final_test[['open', 'final_close_pred', 'close', 'high', 'low', 'volume', 'adjClose', 'adjHigh',
       'adjLow', 'adjOpen', 'adjVolume', 'post_date_count']]
full_final_test.head()

real_price = full_final_test['close']
predicted_price = full_final_test['final_close_pred']
rmse = math.sqrt(mean_squared_error(real_price, predicted_price))
print('Test RMSE: %.3f' % rmse)

plt.figure(figsize=(15,10))
plt.plot(merge_data_tp["close"][:1096])
plt.plot(merge_data_tp["close"][1096:], color = "green")
plt.plot(range(1096,len(merge_data_tp)+1), full_final_test['final_close_pred'],label='forecast',color='red')
plt.title('Comparasion Between Actual and Forecasted Closing Price of Apple Stock Using LSTM Model',fontsize=20)
plt.show()